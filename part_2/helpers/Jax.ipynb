{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hybrid-process",
   "metadata": {},
   "source": [
    "# Installing and using JAX\n",
    "\n",
    "JAX is an auto-differentiation library for native Python and Numpy code which does gradient-based optimization. Auto-differentiation forms the backbone of deep learning libraries like PyTorch.\n",
    "\n",
    "Activate your standard environment from Assignment-3. Then   \n",
    "```\n",
    "pip install --upgrade pip     \n",
    "pip install --upgrade jax jaxlib \n",
    "```\n",
    "\n",
    "[See this](https://github.com/google/jax#installation) for more information. (CPU version should be enough for this project.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-crowd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\"\"\"\n",
    "Use \"jnp\" instead of using \"np\", our favourite numpy library. \n",
    "All functions work as it is (at least that are required for this project).\n",
    "Be careful though:\n",
    "\n",
    "JAX works on python functions that are \"functionally pure\": \n",
    "For the sake of our project, that just means using array datatype everywhere \n",
    "(or 'jnp.array()' in particular) instead of using other datatype, say lists for\n",
    "storing arrays or matrices. Whenever you face some datatype issue with jax, \n",
    "first try to convert it to jax numpy array using `jnp.array()`.\n",
    "\n",
    "Tip: jnp's errors don't seem very readable as compared to np.\n",
    "So use \"np\" first for most of the code and the moment the necessity for \"jnp\" starts, \n",
    "replace all np's with jnp's. Directly replacing should work fine. This is only a tip for easier \n",
    "debugging.\n",
    "\"\"\"\n",
    "from jax import jacfwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some simple function.\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Note that here, I want a derivative of a \"vector\" output function (inputs*a + b is a vector) wrt a input \n",
    "# \"vector\" a at a0: Derivative of vector wrt another vector is a matrix: The Jacobian\n",
    "def simpleJ(a, b, inputs): #inputs is a matrix, a & b are vectors\n",
    "    return sigmoid(jnp.dot(inputs, a) + b)\n",
    "\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "\n",
    "b = jnp.array([0.2, 0.1, 0.3, 0.2])\n",
    "a0 = jnp.array([0.1,0.7,0.7])\n",
    "\n",
    "# Isolate the function: variables to be differentiated from the constant parameters\n",
    "f = lambda a: simpleJ(a, b, inputs) # Now f is just a function of variable to be differentiated\n",
    "\n",
    "J = jacfwd(f)\n",
    "# Till now I have only calculated the derivative, it still needs to be evaluated at a0.\n",
    "J(a0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
